%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}
\label{chapter.introduction}

\section{Definition of AMCs}
Asymmetric Multi-core (AMC) systems architecture is an interesting case of heterogeneous multi-core architecture. 
This multi-core systems architecture features cores with different microarchitectures but with a single Instruction Set Architecture (ISA) and potentially shared memory resources.
Figure~\ref{fig:heterogeneous_classification} shows a classification of multi-core systems and how AMCs relate to the rest of the multi-cores.
\begin{figure}[t]%
	\centering
	\includegraphics[width=0.7\textwidth]{figures/AMC_class.pdf}
	\caption{Classification of multi core system architectures}
	\label{fig:heterogeneous_classification}
\end{figure}
Multi-core systems can be either homogeneous or heterogeneous.
A homogeneous multi-core consists of multiple identical processing cores that have the same micro-architecture and instruction set architecture.
A heterogeneous multi-core can fall into two categories. 
The first category consists multi-cores with different micro-architectures and ISA; systems with GPUs or other compute accelerators fall in this category.
The second category consists of of multi-cores that have different micro-architecture but a single ISA; AMC systems fall in this category.

%it can either consist of multiple cores that have different micro-architecture but a single ISA, or it can consist of multiple cores with different micro-architectures and ISA.
%Examples in the latter category include systems with GPUs or other kinds of accelerators that need to explicitly provide the code that executes on them. 

AMCs have been used in different areas of parallel computing. 
In recent years they made their appearance in the mobile market and are nowadays the most commonly used architecture for mobile processors.
The most widely used AMC architecture for mobile processors is the Arm big.LITTLE architecture~\cite{Greenhalgh2011} which combines low-power simple in-order cores (\emph{little}) with fast out-of-order cores (\emph{big}) to achieve high performance while keeping power dissipation low.
Mobile processors are also utilized in HPC platforms aiming to energy savings~\cite{ARMV8}.
%Asymmetric mobile SoCs combine low-power simple cores (\emph{little}) with fast out-of-order cores (\emph{big}) to achieve high performance while keeping power dissipation low.
Another area where AMCs have been successful is the supercomputing market.
The Sunway TaihuLight supercomputer topped the Top500\footnote{We refer to the list published on November 2017} list in 2016 using AMCs. 
In this setup, big cores, that offer support for speculation to exploit Instruction-Level Parallelism (ILP), run the master tasks such as the OS and runtime system.
Little cores are equipped with wide Single Instruction Multiple Data (SIMD) units and lean pipeline structures for energy efficient execution of compute-intensive code. 

\section{Challenges Using AMCs}
AMCs are easy to program as the programmer does not need to express the functionality of an application for many different ISAs. 
However, there are still challenges that need to be addressed that are mainly related to the performance of the AMC.
%%Add more challenges
Like in other heterogeneous systems, load balancing and scheduling are fundamental challenges that must be addressed to effectively exploit all the resources in AMC platforms~\cite{Suleman:APLOS2009,Fedorova2009,Greenhalgh2011,Joao:ASPLOS2012,Joao:ISCA2013,ARM4HPC_SC13}. 
To preserve load balance, the system has to make sure that the cores of the system remain busy and are fairly loaded depending on their type.
For example running a barrier-based multi-threaded application where each thread runs on one core on an AMC is challenging as there is a high risk of starvation of the fast cores~\cite{AMC_survey}. %ending up in starvation. 
%the fast cores of the system will remain idle after they finish their computations waiting for the slow cores to finish execution.
%This is trivial for multi-threaded applications that run on homogeneous multi-cores but it is not as trivial on an AMC where an equal thread distribution would lead to potential starvation of the fast cores as they 
Additionally, choosing which threads to execute in each type of core requires runtime information such as task dependencies or whether a task is memory or computationally bound.
The above challenges require new asymmetry-aware scheduling approaches in order to take advantage of the heterogeneity.
The state-of-the-art asymmetry-aware scheduling approaches perform thread scheduling within the Linux kernel. 
The most commonly used example in this category is the Global Task Scheduler (GTS)~\cite{?} that is implemented on top of the Linux Completely Fair Scheduler (CFS) and is aware of the asymmetry of the system.
However, such approaches lack flexibility due to the high thread-migration overheads~\cite{AMC_survey}.
%Mobile applications rely on multi-programmed workloads to balance the load in the system, while supercomputer applications rely on hand-tuned code to extract maximum performance. 
%However, these approaches are not always suitable for general-purpose parallel applications.




\section{Solutions}
\kc{mention to Task-based models and process scheduling}
The above challenges can be tackled by using task-based parallel programming models.
These parallel programming models have been widely used during the last decade in the development of parallel applications.
They form an appealing solution as they significantly ease the parallel programming by providing a higher level of abstraction to the programmer through a directive-based or language-based interface.
With these models challenges such as load balancing, scheduling, or thread migration costs are partially solved by using them out-of-the box.
They allow the programmer to split the application in multiple work units called \textit{tasks} (a function can be a task) that can potentially execute simultaneously\footnote{They can execute simultaneously given there are no task dependencies.}.
\begin{lstlisting}[float, emph={void,if,return}, captionpos=b, caption={Example code using the OmpSs task-based programming model.},label=task-based-example, emph={[2]mat}, emphstyle={[3]}, aboveskip={0\baselineskip}, frame=tb, belowskip={0\baselineskip}]
#pragma omp task in(a) out(b)
void task1(int *a, int *b) {
*b = *a/2 + 3 * (*a);
return;
}
#pragma omp task in(c) inout(d)
void task2(int *c, int *d) {
*d = 3 * (*c) + (*d) / 2;
return;
}
int main() { 
int x = 10, y = 5;
for(int i = 0; i <3; i++) {
task1(&x, &y);
task2(&y, &x);
}
return 0;
}
\end{lstlisting}
As an example, Listing~\ref{task-based-example} shows a simple usage of a task-based programming model (the OmpSs programming model~\cite{OmpSs}).
In this code, the \textit{pragma omp task} directives define the functions that act as tasks, namely \texttt{task1} and \texttt{task2}.
The input and output dependencies between these tasks are defined using the \textit{in}, \textit{out} and \textit{inout} clauses.
When these task functions are being called by the main function, the runtime system creates tasks that can run in parallel if there are no task dependencies between them.
The runtime system of the task-based programming model is responsible to maintain software threads and distribute the tasks to the appropriate thread for execution.
Typically the number of software threads that the runtime system maintains is equal to the number of available cores in the system and one thread is bound to each core.
We further explain how the task-based runtime system functions in Section~\ref{sec.background.taskbased}.



%In the last decade, task-parallel programming models have
%emerged as an interesting solution that combines a correct
%orchestration of parallel programs and a reduced impact on
%the complexity of the parallel versions of existing or new
%codes. These models aim at casting a complete computation in
%terms of discrete pieces of code (tasks) with data dependences
%among them with the aid of task annotations provided by the
%programmer, and rely on a runtime task scheduler (or just
%runtime in the following) that orchestrates the correct ordering
%of tasks execution as dependences are satisfied at run time.
%The extension of these programming models and associated
%runtimes to heterogeneous architectures, managing data coherency
%and data transfers among isolated memory spaces has
%been implemented in a number of software efforts, together
%with techniques that drive to performance gains in multi-core,
%many-core, accelerator-based and distributed-memory architectures.
%The necessary efforts to adapt these programming
%models to AMPs is also a topic of interest of recent works [1],
%[2], pursuing the goal of boosting performance by correctly
%mapping critical tasks to the most appropriate element of the
%asymmetric architecture. However, the impact and possibilities
%of task schedulers in terms of improving energy efficiency of
%task-parallel implementations has not been previously studied
%in such a level of detail. As of today, similar efforts, together
%with their impact on performance and energy efficiency, have
%not been ported or adapted to AMPs.


\section{Challenges in Task-based models}

As was illustrated by the example of Listing~\ref{task-based-example} task-based programming models offer a very convenient abstraction layer to the programmer for parallelizing applications.
However, these parallel programming languages exhibit some challenges when moving to asymmetric systems as their runtime system is platform agnostic.
The current scheduling implementation in task-based programming models assumes that all tasks can be evenly distributed among the cores of the system and treats all tasks and cores as equal.
In some cases, a bad scheduling decision on an asymmetric system can lead to significant performance degradation. 
A simple example is when the scheduler decides to execute a task whose output is important for the execution of a set of other tasks to one of the slow cores of the system.
This would lead to keeping the threads idle waiting for the little core to finish the execution of this important task.

A second challenge introduced by the task-based programming models is the significant task generation overhead.
\kc{TODO}


 

% a class of heterogeneous
%parallel architectures in which cores that implement
%different microarchitectures share a common ISA (Instruction
%Set Architecture) and, possibly, a subset of memory resources.
%
%Typically, the available architectural heterogeneity is exploited
%pursuing energy efficiency and performance restrictions on
%heterogeneous software environments. One of the most popular
%implementations of AMPs is the big.LITTLE architectural
%paradigm present in many ARM SoCs (Systems-on-chip),
%that combines a number of high performance ARM Cortex-
%A57/A15 BIG cores with a (possibly different) number of
%energy-efficient ARM Cortex-A53/A7 LITTLE cores. Leveraging
%low-power architectures to the HPC (High Performance
%Computing) arena is one of the main trends in the road
%towards the Exaflop barrier. Among them, ARM Cortex-A
%processors, and more specifically, asymmetric SoCs based on
%this microarchitectural family, are nowadays on the spotlight
%as the most promising architectures to achieve such a goal.

\if 0
----------------------------------------


The use of heterogeneous processing elements is becoming commodity in many aspects of parallel computing. 
From mobile devices to high performance supercomputers heterogeneous multi-processing is attracting a lot of attention as it achieves high performance and at the same time it maintains energy consumption at low levels. 
Asymmetric multi-core systems is an interesting type of heterogeneous multi-processor. 
These systems maintain different types of cores that share the same instruction set architecture.
The different core types are designed to target different optimization points. 
The current state-of-the-art asymmetric system architecture is the Arm big.little architecture~\cite{ARM,Greenhalgh2011}. 
This architecture combines two types of cores: the out-of-order performance-optimized \textit{big} cores and the in-order energy efficient \textit{little} cores.
Even though this asymetric multi-core architecture is mainly used on mobile devices, it is a very interesting approach for HPC as the combnation of fast and slow core units can bring benefits in energy consumption.




% Heterogeneous multicores are a reality in the mobile world.
% 
% Systems composed of big and little cores can switch from low power to high responding operation modes.
% 
% Many researchers are pushing towards building future desktop and HPC systems with mobile chips.
% 
% In this paper, we explore the maturity of these platforms for parallel desktop applications, as well as of the programming models used to program them.

%The conventional wisdom in the high-performance computing (HPC) community predicts the power budget of the first exaFLOPS machines to be of a few tens of megawatts~\cite{Kogge_Exascale_TR08}. Should this prediction be correct, the energy efficiency of such machine needs to be around 20-30 times higher than that of the top supercomputers today. This makes energy efficiency, among others such as reliability, storage and networking, one of the main issues for the design of exascale computing technology.

%Using heterogeneous processing elements is one of the approaches to increase energy efficiency. Different types of processors can be specialized for different types of computation, such as the combination of chip-multiprocessors (CMP) and discrete compute-capable graphics processing units (GPUs). Another approach towards heterogeneity is the use of different processors designed targeting different performance and power optimization points, such as the combination of faster and slower cores in a CMP (e.g., ARM big.LITTLE~\cite{Greenhalgh2011}).


%Heterogeneity in supercomputing nowadays comes mainly from the use of compute accelerators. Generally, general-purpose chip multiprocessors (CMPs) made out of homogeneous cores are combined together with compute-capable graphics processing units (GPUs) that serve as accelerators for compute-intensive parts of an application. This provides a platform in which some parts of the application will run faster on the CMP, and others on the GPU.

%The STI Cell/B.E.~\cite{IntroCell_IBMRandD05} has been one of the major processors with heterogeneous processing elements on the same chip. It was targeted to the gaming market and its compute capabilities attracted HPC users and vendors. It was substantially adopted in supercomputing and was the most energy efficient HPC technology for some years before being discontinued and becoming obsolete. There are no other recent examples of heterogeneous CMPs used in supercomputing. However, heterogeneous CMPs are mainstream in embedded multimedia and mobile processing. Apart from the processor-accelerator scheme using VLIW/DSP cores~\cite{Viper_IEEEDandT01} or GPUs, some heterogeneous CMPs integrate different types of general-purpose cores~\cite{Greenhalgh2011}. Some works envision the adoption of such mobile technology in HPC for energy efficiency~\cite{ARM4HPC_SC13,Abdurachmanov2013}. In this scenario, a heterogeneous CMP with general-purpose fast and slow cores imposes several challenges regarding scheduling that are different to the current accelerator-based heterogeneous systems. Contrarily to the latter, different parts of an HPC application will constantly run faster on fast cores and slower on slow cores.

%Using heterogeneous processing elements is one of the approaches to increase energy efficiency~\cite{Fedorova2009}. Different types of processors can be specialized for different types of computation, such as the combination of chip-multiprocessors (CMP) and discrete compute-capable graphics processing units (GPUs). Another approach towards heterogeneity is the use of different processors designed targeting different performance and power optimization points, such as the combination of faster and slower cores in a CMP (e.g., ARM big.LITTLE~\cite{Greenhalgh2011}).

% <ISPASS17>
The future of parallel computing is highly restricted by energy efficiency~\cite{Kogge_Exascale_TR08}. 
Energy efficiency has become the main challenge for future processor designs, motivating prolific research to face the \emph{power wall}. 
Using heterogeneous processing elements is one of the approaches to increase energy efficiency~\cite{CompCores,hetServers}. 
%Different types of processors can be specialized for different types of computation, such as the combination of general-purpose cores with accelerators such as Graphics Processing Units (GPUs). 
%Another approach towards heterogeneity is the use of asymmetric multi-cores (AMCs).
%An interesting approach towards energy efficiency is the use of asymmetric multi-core (AMC) systems.
Asymmetric multi-core (AMC) systems is an interesting case of heterogeneous systems to utilize for energy efficiency.
These systems maintain different types of cores that support the same instruction-set architecture. 
The different core types are designed to target different (performance or power) optimization points~\cite{Kumar:ISCA2004,Balakrishnan:ISCA2005,Pangaea}. 

AMCs have been mainly deployed for the mobile market. 
Mobile processors are also utilized in HPC platforms aiming to energy savings~\cite{ARMV8}.
Asymmetric mobile SoCs combine low-power simple cores (\emph{little}) with fast out-of-order cores (\emph{big}) to achieve high performance while keeping power dissipation low.
Another area where AMCs have been successful is the supercomputing market.
The Sunway TaihuLight supercomputer topped the Top500 list in 2016 using AMCs. 
In this setup, big cores, that offer support for speculation to exploit Instruction-Level Parallelism (ILP), run the master tasks such as the OS and runtime system.
Little cores are equipped with wide Single Instruction Multiple Data (SIMD) units and lean pipeline structures for energy efficient execution of compute-intensive code. 

%Previous experiences have shown that load balancing and scheduling are fundamental challenges that must be addressed to effectively exploit all the resources in these platforms~\cite{Suleman:APLOS2009,Fedorova2009,Greenhalgh2011,Joao:ASPLOS2012,Joao:ISCA2013,ARM4HPC_SC13}. 
Like in other heterogeneous systems, load balancing and scheduling are fundamental challenges that must be addressed to effectively exploit all the resources in AMC platforms~\cite{Suleman:APLOS2009,Fedorova2009,Greenhalgh2011,Joao:ASPLOS2012,Joao:ISCA2013,ARM4HPC_SC13}. 
Mobile applications rely on multi-programmed workloads to balance the load in the system, while supercomputer applications rely on hand-tuned code to extract maximum performance. 
However, these approaches are not always suitable for general-purpose parallel applications.

In this paper, we evaluate several execution models on an AMC using the PARSEC benchmark suite~\cite{PARSEC3}. 
This suite includes parallel applications from multiple domains such as finance, computer vision, physics, image processing and video encoding. 
We quantify the performance loss of executing the applications \textit{as-is} on all cores in the system. 
These applications were developed on homogeneous platforms and are bound to suffer from load imbalance on parallel regions that statically distribute the work evenly across cores without considering their performance disparity.

To overcome this matter, we consider two possible solutions at the OS and runtime levels to exploit AMCs effectively.
The first solution delegates scheduling to the OS.
We evaluate the built-in heterogeneity-aware OS scheduler currently used in existing mobile platforms that automatically assigns threads to different core types based on CPU utilization. 
%This approach does not require modifying the application, but is limited for high-utilization multithreaded applications.

The second solution is to transfer the responsibility to the runtime system so it dynamically schedules work to different core types based on work progress and core availability. 
%The advantage is that the runtime system has knowledge of the application structure and parallel work boundaries so it can react with certain level of predictability. 
We evaluate the impact of using an inherently load-balanced execution model such that of task-based programming models. 
Recent examples~\cite{Ayguade:TPDS2009, OpenMP4.0:Manual2013, OmpSs_PPL11, vectorMulticore, Bauer.2012.SC,rollback,Vandierendonck:PACT2011, Vandierendonck:Hyperq,spawn} include clauses to specify inter-task dependencies and remove most barriers which are the major source of load imbalance on AMCs.
Another approach of scheduling in the runtime system is to change the existing statically-scheduled work-sharing constructs for the applications implemented in OpenMP to use dynamic scheduling. 
\fi

\section{Thesis Contributions}
\kc{TODO: mention papers and summarize contributions}

The performance of task-based programming models can be boosted by providing sophisticated scheduling policies that take into account the platform's asymmetry.

The contributions made in this thesis rely on the efficient utilization of asymmetric multi-core systems.
First we study the potential of the AMC systems when running out-of-the-box high performance applications.
We compare scheduling on different levels of the software stack and we conclude that using a task-based programming model is indeed the most efficient solution as it allows the runtime system to maintain load balance even if the system is asymmetric.
The paper including this work, named "On the Maturity of Parallel Applications for Asymmetric Multi-Core Processors" is submitted in JPDC journal and is currently under review.
Additionally this work was published as a poster in the conference of Parallel Architectures and Compilation Techniques in 2016 (PACT'16) with the title "POSTER: Exploiting Asymmetric Multi-Core Processors with Flexible System Sofware"

We then approach the two important challenges of using task-based programming models on AMC platforms.
First, is the scheduling problem, which we approach by providing three novel scheduling policies that are aware of the system's asymmetry.
These schedulers have different criteria for rating the importance of the executing tasks of an application.
According to their distinct criteria, each scheduler identifies the \textit{critical tasks} of the application and executes them on the fast cores of the system. 
They then leave the \textit{non-critical tasks} to be executed by the slower cores of the system.
From this contribution we had two publications; first is the "Criticality-Aware Dynamic Task Scheduling for Heterogeneous Architectures" that was presented in International Conference of Supercomputing in 2015 (ICS'15).
The second paper including these contributions was published in the journal Transactions of Parallel and Distributed Systems in 2016 (TPDS'16) with the title "Task Scheduling Techniques for Asymmetric Multi-Core Systems".

The second challenge of task-based programming models is the high task generation overheads introduced while using these parallel programming tools.
We tackle this challenge by proposing a software-hardware co-design that accelerates task generation on a specialized hardware.
In this contribution we implement the TaskGenX runtime system, that decouples the task generation from the other runtime activities and sends it for execution on the runtime optimized accelerator.
Furthermore, we draw the requirements of the hardware accelerator in terms of performance with the hope to influence hardware designers for the implementation of such a component.
The paper including this contribution was published in International Supercomputing Conference on 2018 (ISC'18) with the title "TaskGenX: A Hardware-Software Proposal for Accelerating Task Parallelism".

Apart from scheduling within task-based programming models, this thesis includes the high-level description of a thread scheduler for asymmetric systems targeting this time mobile devices.
Our approach takes scheduling actions depending on the current temperature of the device and manages to increase the frames per second for three game applications while keeping the temperature stable.
This study is briefly described due to NDA agreement as it was part of my internship in Samsung Electronics Research Institute, UK.




Schedulers: CATS, CPATH, HYBRID
Runtime system: TaskGenX
Internship: Real-time scheduler


\section{Thesis Organization}
The rest of this document is organized as follows: 
Chapter~\ref{chapter.background} presents the background of tis thesis.
We first report the characteristics of AMC architectures, we then explain how task-based programming models are organized and how their runtime system operates, we explain a few things about the TaskSim simulator~\cite{AbstrLevels_TACO12}, that is our tool for evaluating the impact of our implementations on larger systems and finally we provide the list of applications used in our evaluation together with a short description for each one.

Chapter~\ref{chapter.study} provides our thorough and detailed study of highly parallel applications on asymmetric systems.
This study is a proof that scheduling on the runtime system is the most efficient way of utilizing an AMC system.
Chapter~\ref{chapter.scheduling} describes and evaluates our three novel scheduling approaches (CATS, CPATH and HYBRID). 
We implement these approaches within the OmpSs programming model and evaluate them using real scientific applications.
To see their impact on larger systems we use TaskSim simulator.

Chapter~\ref{chapter.taskgenx} presents our software-hardware co-design proposal, TaskGenX.
We show the implementation done and we evaluate our proposal using TaskSim simulator and real applications.
Chapter~\ref{chapter.RTS} includes the thread-based scheduling within the Android framework together with its evaluation on a mobile device using three games with intensive graphics.
Chapter~\ref{chapter.related} presents our related work and Chapter~\ref{chapter.conclusions} concludes this thesis.

describes the evaluated AMC processor, while Section~\ref{sec:scheduling} provides information on 
scheduling at the OS and runtime system levels. 
Section~\ref{sec:experimental} describes the experimental framework. 
Section~\ref{sec:evaluation} shows the performance and energy results and associated insights.% of our experiments. 
Finally, Section~\ref{sec:related} discusses related work and Section~\ref{sec:conclusions} concludes this work. 





\iffalse
% <PACT16>

The future of parallel computing is highly restricted by energy 
efficiency~\cite{Kogge_Exascale_TR08}. Energy efficiency has become the main 
challenge for future processor designs, motivating prolific research to face the 
\emph{power wall}. Using heterogeneous processing elements is one of the 
approaches to increase energy efficiency. Different types of processors can 
be specialized for different types of computation, such as the combination of 
general-purpose cores with accelerators such as Graphics Processing Units (GPUs). 
Another approach towards heterogeneity is the use of asymmetric multi-cores 
with different types of cores with the same instruction-set architecture. Different core types 
target different performance and power optimization points for energy
efficiency~\cite{Kumar:ISCA2004,Balakrishnan:ISCA2005}. 

Asymmetric multi-cores have been successfully deployed in the mobile market, where 
low-power simple cores (\emph{little}) are combined with 
high-performance out-of-order cores (\emph{big}). Low demand applications
run on little cores for low power operation and prolong battery life. Demanding
applications, such as games, run on the big cores providing high performance
when needed.

Supercomputing is another market where asymmetric multi-cores have been successful. 
The Sunway TaihuLight supercomputer topped the Top500 list in 2016 using asymmetric multi-cores. 
In this setup, big cores, that offer support for speculation and Instruction-Level Parallelism (ILP), run the master tasks such as the OS and runtime system.
%system, as these tasks require support for speculation and Instruction-Level Parallelism (ILP) 
%exploitation of codes with complex control flow.
Little cores are equipped with wide Single Instruction Multiple Data (SIMD) units and lean pipeline structures for energy efficient execution of compute-intensive codes. 

Previous experiences have shown that load balancing and scheduling are fundamental challenges that 
must be addressed to effectively exploit all the resources in these 
platforms~\cite{Suleman:APLOS2009,Fedorova2009,Greenhalgh2011,Joao:ASPLOS2012,Joao:ISCA2013,
ARM4HPC_SC13}. 
Mobile applications rely on multi-programmed workloads to balance the load in the 
system, while supercomputer applications rely on hand-tuned code to extract maximum 
performance. However, these approaches are not always suitable for general-purpose parallel 
applications.
%In a first generation of asymmetric multi-cores, the system could switch from low power to high responding operation modes, activating or de-activating the cluster of big or little cores accordingly~\cite{ARM}. In a second generation of asymmetric multi-core processors, all the cores can run simultaneously to further improve the peak performance of these systems~\cite{samsung}.

%Many researchers are pushing towards building future parallel systems with asymmetric multi-cores~\cite{Suleman:APLOS2009,Fedorova2009, Greenhalgh2011, Joao:ASPLOS2012,Joao:ISCA2013} and even mobile chips~\cite{ARM4HPC_SC13}. However, it is unclear if current parallel applications will benefit from these asymmetric platforms. Load balancing and scheduling are two of the main challenges in utilizing such heterogeneous platforms, as the programmer has to consider them from the very beginning to obtain an efficient parallelization.

%In this paper, we evaluate for the first time the suitability of currently available mobile asymmetric multi-core platforms for general purpose computing. First, we demonstrate that out-of-the-box parallel applications do not run efficiently on asymmetric multi-cores. Fully exploiting the computational power of these processors is challenging as the asymmetry in the system can lead to load imbalance, undermining the scalability of the parallel application. Consequently, only applications that incorporate user-defined load balancing mechanisms can benefit immediately from asymmetric multi-cores.

In this paper, we evaluate several execution models on an asymmetric multi-core
using the PARSEC benchmark suite. This suite includes parallel applications from multiple domains 
such as finance, computer vision, physics, image processing and video encoding. We first quantify 
the performance loss of executing the applications \textit{as-is} on all cores 
in the system. These applications were developed on homogeneous platforms and are bound to suffer from
load imbalance on parallel regions that statically distribute the work
evenly across cores without considering their performance disparity.

Then, we evaluate several solutions at the OS and runtime level that require different
levels of user intervention to exploit asymmetric multi-cores effectively. The first
solution delegates scheduling to the OS. We evaluate the heterogeneity-aware
OS scheduler used in existing mobile platforms that assigns threads to different
core types based on CPU utilization. This requires no modification of the
application, but has limited capability for high-utilization multithreaded applications.

%on an ARM big.LITTLE asymmetric multi-core platform 

%When load-balancing techniques are not included in the original application, we evaluate alternative solutions that, without relying on the programmer, can leverage the opportunities that asymmetric systems offer. In particular, we evaluate a state of the art dynamic scheduler at the Operating System (OS) level that is aware of the characteristics of each core type. This scheduler effectively exploits the system by running high CPU utilization processes on the big cores and low CPU utilization processes on the little cores.

The second solution is to transfer the responsibility to the runtime system so it 
dynamically schedules work to different core types based on work progress and core 
availability. The advantage is that the runtime system has knowledge of the application 
structure and parallel work boundaries so it can react with certain level of predictability. 
We evaluate dynamic scheduling on top of the existing work-sharing constructs in the applications 
with an OpenMP statically-scheduled implementation available. This requires code transformations 
that are straightforward in many cases.

Finally, we evaluate the impact of using an inherently load-balanced execution model such 
that of task-based programming models. 
Recent examples~\cite{Ayguade:TPDS2009, OpenMP4.0:Manual2013, OmpSs_PPL11, Zuckerman:EXADAPT2011, Bauer.2012.SC, Vandierendonck:PACT2011, Vandierendonck:Hyperq} 
include clauses to specify inter-task dependences and remove most barriers which are the major 
source of load imbalance on asymmetric multi-cores.

%and let the runtime system to track dependences between tasks. When these dependences are satisfied, tasks are dynamically scheduled, effectively balancing the workload.

This paper quantifies the effectiveness of these solutions at different levels of the software stack
with a comprehensive evaluation of representative parallel applications on a real 
asymmetric multi-core platform: the Odroid-XU3 development board. This platform features an 
eight-core Samsung Exynos 5422 chip with ARM big.LITTLE architecture with 
four out-of-order Cortex-A15 and four in-order Cortex-A7 cores.

The rest of this document is organized as follows: Section~\ref{sec:background} describes the 
evaluated asymmetric multi-core processor, while Section~\ref{sec:scheduling} offers information on 
dynamic schedulers at the OS and runtime system levels. Section~\ref{sec:experimental} 
describes the experimental framework. Section~\ref{sec:evaluation} shows the performance 
and energy results and associated insights of our experiments. Finally, 
Section~\ref{sec:related} discusses related work and Section~\ref{sec:conclusions} concludes 
this work. 
\fi
%\begin{itemize}
% \item Out-of-the-box applications obtain the best average performance when running only on the aggressive out-of-order cores. Many of these applications are not ready to fully exploit asymmetric multi-cores as they suffer from load imbalance due to the system's heterogeneity. As a result, an average 12\% performance degradation is obtained when using all the cores in the system instead of the four out-of-order cores. 
% \item For the OS scheduler it takes three additional little cores on average to reach the performance obtained with four out-of-order cores. This is observed in most evaluated applications; the addition of little cores to a homogeneous big-core system is degrading performance. Specifically, this slowdown is observed to be 22\% on average when one little core is added to a system that consists of four big cores.
%% applications have 22\% better performance on four big cores compared to their performance on a system with four big and one little cores. 
%When adding four little cores the OS scheduler reduces total execution time by 5.3\% but contrarily to this, it is shown how the runtime system scheduling constantly improves performance by up to 16\%.
%  
%% \item Dynamic scheduling techniques at OS level can turn the tables, reducing the total execution time by 5.3\% when adding four little cores to a system with four big cores. The dynamic scheduler in the runtime system can further improve the final performance by fully utilizing all the resources in the system. This approach reaches an average 13\% speedup, and leads to the most energy efficient solution, as the Energy-Delay Product (EDP) is reduced by 40\% in this configuration.
% \item Moreover, the energy delay product (EDP) results show that the optimal solution taking into account both energy and performance remains the runtime system scheduling.
%%  In systems with a given number of out-of-order cores, adding extra in-order cores can further boost the performance of the application with the appropriate software support (at the application, runtime or OS level). As a result, the energy consumption of parallel applications running on those systems can be reduced by XXX\% on a system with four in-order and four out-of-order cores.
% \item Finally, we evaluate the usefulness of little cores to off-load runtime system activities. Similarly to the assistant core in the IBM Blue Gene Q and the Fujitsu SPARC64 XIfx processors~\cite{BG-Q:HotChips2011, Fujitsu:HotChips2014}, we explore the possibilities of devoting a little or big core to the runtime system activity. In general, we observe that the noise introduced by the runtime system does not degrade the performance of the parallel application. Thus, we can make use of this assistant core to also run user tasks, increasing the final performance of the system.
%\end{itemize}


% </PACT16>

% We describe a set of configurations for our scheduler regarding the work stealing capabilities of the different core types and the flexibility to define a task as critical or non-critical. 
 
% We implement this scheduler in OmpSs and evaluate its effectiveness on different numbers of cores and shares of fast and slow cores on a real system. 
 
% We also evaluate the effectiveness of our scheduler depending on the speed ratio between fast and slow cores using simulation.  The results show that the effectiveness of our scheduler increases with larger numbers of fast cores over slow cores, and with larger differences of performance between fast and slow cores.

%Finally, we provide a set of recommendations on how to configure our scheduler to get the best results depending on the target system size and configuration.
